{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tutorial de  Julia para Otimização\n",
    "## Escola de Verão - EMap/FGV\n",
    "## Aula 04 - NLPModels e Comparação de Algortimos\n",
    "\n",
    "### Ministrante\n",
    "- Luiz-Rafael Santos ([LABMAC/UFSC/Blumenau](http://labmac.mat.blumenau.ufsc.br))\n",
    "    * Email para contato: [l.r.santos@ufsc.br](mailto:l.r.santos@ufsc.br) ou [lrsantos11@gmail.com](mailto:lrsantos11@ufsc.br)\n",
    "\t- Repositório do curso no [Github](https://github.com/lrsantos11/Tutorial-Julia-Opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Pacotes para contrução e comparação de algoritmos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* [JuliaSmoothOptimizers (JSO)](https://juliasmoothoptimizers.github.io/) coleção de pacotes em Julia para desenvolvimento, teste e *benchmark* de algoritmos de otimização (não-linear).\n",
    "    \n",
    "    * Modelagem\n",
    "        * [NLPModels](https://github.com/JuliaSmoothOptimizers/NLPModels.jl): API para representar problemas de otimização `min f(x) s.t. l <= c(x) <= u`\n",
    "    * Benchmark\n",
    "        * [BenchmarkProfiles](https://github.com/JuliaSmoothOptimizers/BenchmarkProfiles.jl)\n",
    "    * Respositórios de problemas\n",
    "        * [CUTEst.jl](https://github.com/JuliaSmoothOptimizers/CUTEst.jl): interface para o [CUTEst](http://ccpforge.cse.rl.ac.uk/gf/project/cutest/wiki), repositório de problemas de otimização para teste  comparação de algoritmos de otimização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "pkg\"activate ../.\"\n",
    "pkg\"instantiate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instalando os pacotes\n",
    "\n",
    "* Vamos instalaros pacotes:\n",
    "    * `NLPModels` para modelagem\n",
    "    * `NLPModelsIpopt` `NLPModelsJuMP`que fazem interface entre NLPModels a Ipopt e JuMP\n",
    "    * `CUTEst` para permitir usar os problemas da biblioteca [CUTEst](https://github.com/ralna/CUTEst/wiki)\n",
    "\t* `BenchmarkProfiles` para comparação de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pkg\"add NLPModels NLPModelsIpopt NLPModelsJuMP CUTEst BenchmarkProfiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Plots, LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLPModels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interfaces do NLPModels\n",
    "#### [Interface Internas](https://juliasmoothoptimizers.github.io/NLPModels.jl/stable/#Internal-Interfaces)\n",
    "\n",
    " - `ADNLPModel`: Usa\n",
    "   [`ForwardDiff`](https://github.com/JuliaDiff/ForwardDiff.jl) para computar derivadas. Simples mas não tão eficiente\n",
    "   for larger problems.\n",
    " - `SlackModel`: Cria problemas com restrições de igualdade e restrições de caixa a partir de um NLPModel existente.\n",
    " - `LBFGSModel`: Cria modelo usando a aproximação LBFGS para a Hessiana a partir de um NLPModel existente.\n",
    " - `ADNLSModel`: Similar a  `ADNLPModel`, mas para qudarados mínimos não-lineares (nonlinear least squares)\n",
    "\n",
    "#### [Interface Externas](https://juliasmoothoptimizers.github.io/NLPModels.jl/stable/#External-Interfaces)\n",
    "\n",
    " - `AmplModel`: Definida em \n",
    "   [`AmplNLReader.jl`](https://github.com/JuliaSmoothOptimizers/AmplNLReader.jl)\n",
    "   para problemas modelados com [AMPL](https://ampl.com)\n",
    " - `CUTEstModel`: Definida em \n",
    "   [`CUTEst.jl`](https://github.com/JuliaSmoothOptimizers/CUTEst.jl) para problemas da biblioteca [CUTEst](https://github.com/ralna/CUTEst/wiki).\n",
    " - [`MathOptNLPModel`](https://github.com/JuliaSmoothOptimizers/NLPModelsJuMP.jl) e [`MathOptNLSModel`](https://github.com/JuliaSmoothOptimizers/NLPModelsJuMP.jl)\n",
    "   para problemas modelados usando [JuMP.jl](https://github.com/jump-dev/JuMP.jl) e [MathOptInterface.jl](https://github.com/jump-dev/MathOptInterface.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* NLPModels considera o problema de otimização dado da forma\n",
    "\n",
    "$$\\tag{P}\n",
    "\\begin{aligned}\n",
    "\\min \\quad & f(x) \\\\\n",
    "& c_L \\leq c(x) \\leq c_U \\\\\n",
    "& \\ell \\leq x \\leq u.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exemplo - Função de Rosenbrock\n",
    "Vamos novamente usar a função (não-linear) de Rosenbrock\n",
    "$$f(x) = (1-x_1)^2 + 100(x_2-x_1^2)^2$$\n",
    "para testar o pacote `NLPModels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f(x) = (1-x[1])^2 + 100(x[2] - x[1]^2)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "com ponto inicial $(-1.2,1.0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x0 = [-1.2,1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Vamos criar o modelo usando a interface `ADNLPModel`\n",
    "\n",
    "O problema (P) em Julia será escrito como\n",
    "```julia\n",
    " min  f(x)\n",
    "s. a  lcon ≤ c(x) ≤ ucon\n",
    "      lvar ≤   x  ≤ uvar\n",
    "```\n",
    "* As entradas de `ADNLPModel` podem ser\n",
    "\n",
    "```julia\n",
    "    ADNLPModel(f, x0)\n",
    "    ADNLPModel(f, x0, lvar, uvar)\n",
    "    ADNLPModel(f, x0, c, lcon, ucon)\n",
    "    ADNLPModel(f, x0, lvar, uvar, c, lcon, ucon)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using NLPModels\n",
    "adnlp = ADNLPModel(f,x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [API](https://juliasmoothoptimizers.github.io/NLPModels.jl/stable/api/#)\n",
    "\n",
    "\n",
    "\n",
    "* Em otimização, como vimos, além dos valores $f(x)$ e $c(x)$ no ponto $x$ também precisamos das derivadas relacionadas, isto é,\n",
    "\n",
    "- $\\nabla f(x)$, $\\nabla^2 f(x)$, $J_c(x) = \\nabla c(x)^T$; \n",
    "- $\\nabla^2 f(x) + \\sum_{i=1}^m \\lambda_i \\nabla^2 c_i(x)$,\n",
    "  Hessiana da função Lagrangeana no ponto $(x,\\lambda)$.\n",
    "> Para todas as funções disponibilizadas pelo pacote veja o [Guia de Referências](https://juliasmoothoptimizers.github.io/NLPModels.jl/stable/api/#Reference-guide) do API\n",
    "\n",
    "* Vejam alguas que podemos usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "@show obj(adnlp, adnlp.meta.x0)\n",
    "@show grad(adnlp, adnlp.meta.x0)\n",
    "@show hess(adnlp, adnlp.meta.x0)\n",
    "@show objgrad(adnlp, adnlp.meta.x0)\n",
    "@show hprod(adnlp, adnlp.meta.x0, ones(2))\n",
    "@show H = hess_op(adnlp, adnlp.meta.x0)\n",
    "H * ones(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(adnlp.counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Função `hess` em detalhes\n",
    "\n",
    "* Apenas devolve o triangulo inferior da matriz Hessiana.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Hx = hess(adnlp,adnlp.meta.x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "HxS = Symmetric(Hx,:L) #ou Hermitian(Hx,:L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "F = cholesky(HxS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Usando modelos em JuMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using JuMP, NLPModelsJuMP\n",
    "rosen = Model()\n",
    "\n",
    "@variable(rosen,x[1:2])\n",
    "\n",
    "@NLobjective(rosen,Min,(1-x[1])^2 + 100(x[2]-x[1]^2)^2)\n",
    "print(rosen)\n",
    "\n",
    "jpnlp = MathOptNLPModel(rosen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Usando a biblioteca CUTEst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Um pouco mais sobre [CUTEst](https://github.com/ralna/CUTEst/wiki)\n",
    "    > CUTEst, a Constrained and Unconstrained Testing Environment on steroids, is the latest evolution of CUTE, the constrained and unconstrained testing environment for numerical optimization.\n",
    "\n",
    "* A interface com Julia que estamos usando permite acessar os problemas da CUTEst + outras bibliotecas (manualmente) que estão descritas no formato SIF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using CUTEst\n",
    "\n",
    "cutenlp = CUTEstModel(\"ROSENBR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTEst.select()# Sempre preciso finalizar o modelo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problmes2D = CUTEst.select(max_var=2,contype=:unc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vamos usar nosso método de Newton\n",
    "\n",
    "\n",
    "* Vamos voltar ao método de Newton que implementamos na Aula 02 porém \n",
    "    * o adaptaremos para uso do NLPModels\n",
    "    * usaremos fatoração de Cholesky pra resolver o sistema linear de Newton\n",
    "    * adicionaremos uma busca linear inexata com [Armijo com backtracking](https://en.wikipedia.org/wiki/Backtracking_line_search) para garantir descenso suficiente da direção de Newton $d_k$:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function newton(nlp::AbstractNLPModel; itmax = 10_000,ε = 1e-6)\n",
    "\tk = 0\n",
    "    xₖ = nlp.meta.x0\n",
    "    gradₖ = grad(nlp,xₖ)\n",
    "    while k <= itmax && norm(gradₖ) >= ε\n",
    "        Hx = hess(nlp,xₖ)\n",
    "        F = cholesky(Symmetric(Hx,:L))\n",
    "        d = - (F \\ gradₖ)\n",
    "    \txₖ = xₖ + d \n",
    "        gradₖ = grad(nlp,xₖ)\n",
    "    \tk += 1\n",
    "    end\n",
    "    return xₖ, k\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show xₖ, k = newton(adnlp)\n",
    "print(adnlp.counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpnlp.meta.x0\n",
    "@show xₖ, k = newton(jpnlp)\n",
    "print(jpnlp.counters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutenlp = CUTEstModel(\"ROSENBR\")\n",
    "@show xₖ, k = newton(cutenlp)\n",
    "print(cutenlp.counters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Armijo com backtracking.**\n",
    ">   Enquanto  $f(x_k + \\alpha_kd_k) > f(x_k) + \\alpha_k\\rho\\nabla f(x_k)^Td_k $ escolha novo  $\\alpha_k \\in [0.1\\alpha_k,0.9\\alpha_k]$, para $\\rho > 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function newton_bt(nlp::AbstractNLPModel; itmax = 10_000,ε = 1e-6,ρ::Float64 = 0.5)\n",
    "\tk = 0\n",
    "    xₖ = nlp.meta.x0\n",
    "    fₖ = obj(nlp,xₖ)\n",
    "    gradₖ = grad(nlp,xₖ)\n",
    "    while k <= itmax && norm(gradₖ) >= ε\n",
    "        Hx = hess(nlp,xₖ)\n",
    "        F = cholesky(Symmetric(Hx,:L))\n",
    "        dₖ = - (F \\ gradₖ)\n",
    "        αₖ = 1.0\n",
    "        xtrial = xₖ + αₖ*dₖ \n",
    "        fα = obj(nlp,xtrial)\n",
    "        while fα > fₖ + ρ * αₖ * dot(gradₖ,dₖ) \n",
    "            αₖ *= 0.5\n",
    "            xtrial = xₖ + αₖ*dₖ \n",
    "            fα = obj(nlp,xtrial)\n",
    "        end\n",
    "        xₖ = xtrial\n",
    "        fₖ = obj(nlp,xₖ)\n",
    "        gradₖ = grad(nlp,xₖ)\n",
    "    \tk += 1\n",
    "    end\n",
    "    return xₖ, k\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_bt(cutenlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Um exemplo que Newton BT não funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"HIMMELBB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_bt(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalize(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = CUTEstModel(\"BOX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_bt(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BenchmarkOrofiles\n",
    "O pacote `BenchmarkProfiles` fornece uma fácil maneira de construir os Performance Profiles de Dolan e Moré. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkProfiles\n",
    "gr()\n",
    "T = 10 * rand(25,3);\n",
    "plt = performance_profile(T, [\"Solver 1\", \"Solver 2\", \"Solver 3\"], title=\"Celebrity Deathmatch\")\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
